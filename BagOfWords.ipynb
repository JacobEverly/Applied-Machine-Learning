{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebd8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import heapq\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad1001",
   "metadata": {},
   "source": [
    "# 1. Binary Classification on Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304e491",
   "metadata": {},
   "source": [
    "## a. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5faf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2c77d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc072763",
   "metadata": {},
   "source": [
    "### 1. There are 7613 training data points and 3263 test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745fca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4296597924602653\n"
     ]
    }
   ],
   "source": [
    "target = df_train['target']\n",
    "real = [t for t in target if t == 1]\n",
    "\n",
    "print(len(real) / target.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae9365",
   "metadata": {},
   "source": [
    "### 2. ~ 43% are of real disasters and ~57% are not of real disasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a902632",
   "metadata": {},
   "source": [
    "## b. Split the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ba513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.target\n",
    "X = df_train.drop('target', axis=1)\n",
    " \n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83e191",
   "metadata": {},
   "source": [
    "## c. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493b37b",
   "metadata": {},
   "source": [
    "We converted all the words to lowercase in order to properly be processed by the lemmatizer (for example, we noticed that Dogs was not converted to Dog in the lemmatizer step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b5e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['text'].map(lambda s: s.lower())\n",
    "X_dev['text'] = X_dev['text'].map(lambda s: s.lower())\n",
    "df_test['text'] = df_test['text'].map(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9474f",
   "metadata": {},
   "source": [
    "We lemmatized the words in order to consolidate the features and combine the weights of related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddd19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(s):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenization = nltk.word_tokenize(s)\n",
    "    for w in tokenization:\n",
    "        s = s.replace(w, wordnet_lemmatizer.lemmatize(w))\n",
    "    return s\n",
    "    \n",
    "X_train['text'] = X_train['text'].map(lambda s: lemmatize(s))\n",
    "X_dev['text'] = X_dev['text'].map(lambda s: lemmatize(s))\n",
    "df_test['text'] = df_test['text'].map(lambda s: lemmatize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecae2de",
   "metadata": {},
   "source": [
    "We stripped punctuation to remove noisy text that does not contribute to the model effectiveness (an example is an exclamation point which could indicate an emergency but it could also indicate excitement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649ea13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "X_train['text'] = X_train['text'].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
    "X_dev['text'] = X_dev['text'].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
    "df_test['text'] = df_test['text'].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffde2ab",
   "metadata": {},
   "source": [
    "We stripped the stop words in the call to CountVectorizer below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64a36d",
   "metadata": {},
   "source": [
    "We stripped direct mentions (a) because they are mainly for individuals and do not indicate the presence of an emergency.\n",
    "\n",
    "We stripped urls because it is hard to capture value from a URL and they likely won't make it into our vocabulary in the CountVectorizer.  Additionally, in an emergency people might not have the time to copy and paste a URL in the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f086b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove @...\n",
    "X_train['text'] = X_train['text'].map(lambda s: re.sub(r'\\B\\@\\w+', '', s))\n",
    "X_dev['text'] = X_dev['text'].map(lambda s: re.sub(r'\\B\\@\\w+', '', s))\n",
    "df_test['text'] = df_test['text'].map(lambda s: re.sub(r'\\B\\@\\w+', '', s))\n",
    "\n",
    "# remove urls\n",
    "X_train['text'] = X_train['text'].map(lambda s: re.sub(r'http\\S+', '', s))\n",
    "X_dev['text'] = X_dev['text'].map(lambda s: re.sub(r'http\\S+', '', s))\n",
    "df_test['text'] = df_test['text'].map(lambda s: re.sub(r'http\\S+', '', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b27882",
   "metadata": {},
   "source": [
    "We removed numerical data because it was being captured in our CountVectorizer and we don't think it correlates with an emergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c93217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numerical data\n",
    "X_train['text'] = X_train['text'].map(lambda s: re.sub(r'[\\d-]', '', s))\n",
    "X_dev['text'] = X_dev['text'].map(lambda s: re.sub(r'[\\d-]', '', s))\n",
    "df_test['text'] = df_test['text'].map(lambda s: re.sub(r'[\\d-]', '', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b5321",
   "metadata": {},
   "source": [
    "We also decided to drop unnecessary columns because they do not contribute to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f64d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "X_train.drop('id', axis=1, inplace=True)\n",
    "X_train.drop('keyword', axis=1, inplace=True)\n",
    "X_train.drop('location', axis=1, inplace=True)\n",
    "\n",
    "X_dev.drop('id', axis=1, inplace=True)\n",
    "X_dev.drop('keyword', axis=1, inplace=True)\n",
    "X_dev.drop('location', axis=1, inplace=True)\n",
    "\n",
    "test_id_cols = df_test.id\n",
    "df_test.drop('id', axis=1, inplace=True)\n",
    "df_test.drop('keyword', axis=1, inplace=True)\n",
    "df_test.drop('location', axis=1, inplace=True)\n",
    "\n",
    "# save this for testing at the end\n",
    "X_total = pd.concat([X_train, X_dev])\n",
    "y_total = pd.concat([y_train, y_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68603e91",
   "metadata": {},
   "source": [
    "## d. Bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07586040",
   "metadata": {},
   "source": [
    "We decided to use M=10 as the threshold because it gave us the best results in e) and f).  Additionally, given the sample size of ~5000 points for the training set we believe if a word appears in at least 10 different tweets it is important to consider in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a855f",
   "metadata": {},
   "source": [
    "Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20656e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (vectorized words) in training set: 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>abc</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accident</th>\n",
       "      <th>account</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>youll</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  abc  ablaze  about  absolutely  accident  account  across  act  \\\n",
       "0    0    0       0      0           0         0        0       0    0   \n",
       "1    0    0       0      0           0         0        0       0    0   \n",
       "2    0    0       0      0           0         0        0       0    0   \n",
       "3    0    0       0      0           0         0        0       0    0   \n",
       "4    0    0       0      0           0         0        0       0    0   \n",
       "\n",
       "   action  ...  you  youll  your  youre  yours  yourself  youth  youtube  yr  \\\n",
       "0       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "1       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "2       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "3       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "4       0  ...    1      0     1      0      0         0      0        0   0   \n",
       "\n",
       "   zone  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 1140 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, min_df=10, strip_accents='ascii', stop_words=['the', 'and', 'or', 'an'])\n",
    "count_vectorized = vectorizer.fit_transform(X_train['text'])\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "count_vectorized_df = pd.DataFrame(data=count_vectorized.toarray(), columns=columns)\n",
    "print(f'Number of features (vectorized words) in training set: {len(count_vectorized.toarray()[0])}')\n",
    "count_vectorized_df.head()\n",
    "\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229635c",
   "metadata": {},
   "source": [
    "Development Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17217c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (vectorized words) in development set: 1140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>abc</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accident</th>\n",
       "      <th>account</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>youll</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  abc  ablaze  about  absolutely  accident  account  across  act  \\\n",
       "0    0    0       0      0           0         0        0       0    0   \n",
       "1    0    0       0      0           0         0        0       0    0   \n",
       "2    0    0       0      0           0         0        0       0    0   \n",
       "3    0    0       0      0           0         0        0       0    0   \n",
       "4    0    0       0      0           0         0        0       0    0   \n",
       "\n",
       "   action  ...  you  youll  your  youre  yours  yourself  youth  youtube  yr  \\\n",
       "0       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "1       0  ...    1      0     0      0      0         0      0        0   0   \n",
       "2       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "3       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "4       0  ...    0      0     0      0      0         0      0        0   0   \n",
       "\n",
       "   zone  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 1140 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorized_dev = vectorizer.transform(X_dev['text'])\n",
    "count_vectorized_dev_df = pd.DataFrame(data=count_vectorized_dev.toarray(), columns=columns)\n",
    "print(f'Number of features (vectorized words) in development set: {len(count_vectorized_dev.toarray()[0])}')\n",
    "count_vectorized_dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9f7f3",
   "metadata": {},
   "source": [
    "## e. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c022014",
   "metadata": {},
   "source": [
    "### i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac964996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewcline/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='none').fit(count_vectorized_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea1647",
   "metadata": {},
   "source": [
    "Training Set F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933081ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8628687232605269\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(count_vectorized_df)\n",
    "print(f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2159d",
   "metadata": {},
   "source": [
    "Development Set F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f25a580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731083844580777\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(count_vectorized_dev_df)\n",
    "print(f1_score(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b741eef",
   "metadata": {},
   "source": [
    "It seems like it is overfitting due to the noticeable difference in performance between the training and development sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e6aec",
   "metadata": {},
   "source": [
    "### ii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd3e1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_l1 = LogisticRegression(penalty='l1', solver='liblinear').fit(count_vectorized_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d18479",
   "metadata": {},
   "source": [
    "Training Set F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1b23e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8230376219228983\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l1.predict(count_vectorized_df)\n",
    "print(f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91bb79a",
   "metadata": {},
   "source": [
    "Development Set F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc8b6a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482327351821643\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l1.predict(count_vectorized_dev_df)\n",
    "print(f1_score(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec522425",
   "metadata": {},
   "source": [
    "### iii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64319138",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_l2 = LogisticRegression(penalty='l2').fit(count_vectorized_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ae8f6",
   "metadata": {},
   "source": [
    "Training Set F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e083d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830484988452656\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l2.predict(count_vectorized_df)\n",
    "print(f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82c958",
   "metadata": {},
   "source": [
    "Development Set F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c2d44d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7508055853920516\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l2.predict(count_vectorized_dev_df)\n",
    "print(f1_score(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139c0d3",
   "metadata": {},
   "source": [
    "### iv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112b575",
   "metadata": {},
   "source": [
    "The first classifier performed best on the training dataset but it seems to be slightly overfitting since the first classfier performs the worst on the development dataset.  However, the L2 classifier performed the best on the develompent dataset which indicates the regularization did in fact reduce overfitting.  Additionally, the L2 classifier performed well enough on the training dataset as well which indicates this classifier performed the best overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04852881",
   "metadata": {},
   "source": [
    "### v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28318de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important words indicating a tweet is an emergency: ['bombing' 'hiroshima' 'wildfire']\n",
      "Most important words indicating a tweet is NOT an emergency: ['wedding' 'ebay' 'finally']\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = sorted(range(len(clf_l1.coef_[0])), key=lambda k: clf_l1.coef_[0][k])\n",
    "largest_indices = sorted_indices[:3]\n",
    "smallest_indices = sorted_indices[-3:]\n",
    "\n",
    "print(f\"Most important words indicating a tweet is an emergency: {columns[smallest_indices]}\")\n",
    "print(f\"Most important words indicating a tweet is NOT an emergency: {columns[largest_indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8f8da",
   "metadata": {},
   "source": [
    "## f. Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21790eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: lecture notes - https://github.com/kuleshov/cornell-cs5785-2022-applied-ml/blob/main/lecture-notes/lecture6-naive-bayes.ipynb\n",
    "def get_naive_bayes_theta_vals(X_df, y_df, K):\n",
    "    n, d = X_df.shape\n",
    "    alpha = 1\n",
    "\n",
    "    psis, phis = np.zeros([K, d]), np.zeros([K])\n",
    "\n",
    "    y_arr = np.array(y_df)\n",
    "    X_arr = X_df.to_numpy()\n",
    "\n",
    "    for k in range(K):\n",
    "        X_k = X_arr[y_arr == k]\n",
    "        psis[k] = (np.sum(X_k, axis=0) + alpha) / (X_k.shape[0] + 2*alpha)   # Laplace smoothing\n",
    "        phis[k] = X_k.shape[0] / float(n)\n",
    "        \n",
    "    return psis, phis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d2826f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: lecture notes - https://github.com/kuleshov/cornell-cs5785-2022-applied-ml/blob/main/lecture-notes/lecture6-naive-bayes.ipynb\n",
    "def naive_bayes_predict(x, psis, phis, K):\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    \n",
    "    logpy = np.log(phis).reshape([K, 1])\n",
    "    logpxy = x * np.log(psis) + (1 - x) * np.log(1 - psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da27e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "psis, phis = get_naive_bayes_theta_vals(count_vectorized_df, y_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae076d",
   "metadata": {},
   "source": [
    "Training Set F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d53ac220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7702417272940624\n"
     ]
    }
   ],
   "source": [
    "idx, logpyx = naive_bayes_predict(np.array(count_vectorized_df), psis, phis, 2)\n",
    "print(f1_score(y_train, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea0f2c",
   "metadata": {},
   "source": [
    "Development Set F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d816565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406181015452538\n"
     ]
    }
   ],
   "source": [
    "idx, logpyx = naive_bayes_predict(np.array(count_vectorized_dev_df), psis, phis, 2)\n",
    "print(f1_score(y_dev, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de19b70",
   "metadata": {},
   "source": [
    "## g. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7808513",
   "metadata": {},
   "source": [
    "The L2-regularized logistic regression classifier and the Naive Bayes classifier both performed the best in determining whether a tweet was an emergency or not.  \n",
    "\n",
    "The pros of using a discriminative model is that it is more robust to outliers, requires less data, and is computationally cheaper.  The cons of using a discriminative model include it is more difficult to interpret than generative models.\n",
    "\n",
    "The pros of using a generative model is that it is good at unsupervised learning and it gives us a good idea of the underlying data distribution.  A con is that it is more computationally expensive. \n",
    "\n",
    "The assumption of Naive Bayes is that each feature (word in this example) is independent of one another.  From our results, it looks like the Naive Bayes classifier and the Logistic Regression classifier performs similary so that confirms it is valid and efficient to use Bernoulli Naive Bayes classifier for natural language texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5e171",
   "metadata": {},
   "source": [
    "## h. N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cad1f",
   "metadata": {},
   "source": [
    "We decided to use M=5 because it is half of our optimal value from the 1-gram version (M=10) and the frequency of pairs of words is lower than the frequency of single words so wanted the M-value to be low enough to capture a good amount of the 2-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeecb15",
   "metadata": {},
   "source": [
    "Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c58b6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total features in training set: 3228\n",
      "Number of 2-gram features in training set: 1250\n",
      "Number of 1-gram features in training set: 1978\n",
      "10 2-gram features: ['aba woman', 'abc news', 'able to', 'about it', 'about to', 'about trapped', 'according to', 'account of', 'action year', 'added video']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abcnews</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>able to</th>\n",
       "      <th>about</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth saved</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube playlist</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>youve</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  aba woman  abandoned  abc  abc news  abcnews  ablaze  able  able to  \\\n",
       "0    0          0          0    0         0        0       0     0        0   \n",
       "1    0          0          0    0         0        0       0     0        0   \n",
       "2    0          0          0    0         0        0       0     0        0   \n",
       "3    0          0          0    0         0        0       0     0        0   \n",
       "4    0          0          0    0         0        0       0     0        0   \n",
       "\n",
       "   about  ...  youth  youth saved  youtube  youtube playlist  youtube video  \\\n",
       "0      0  ...      0            0        0                 0              0   \n",
       "1      0  ...      0            0        0                 0              0   \n",
       "2      0  ...      0            0        0                 0              0   \n",
       "3      0  ...      0            0        0                 0              0   \n",
       "4      0  ...      0            0        0                 0              0   \n",
       "\n",
       "   youve  yr  yr old  yyc  zone  \n",
       "0      0   0       0    0     0  \n",
       "1      0   0       0    0     0  \n",
       "2      0   0       0    0     0  \n",
       "3      0   0       0    0     0  \n",
       "4      0   0       0    0     0  \n",
       "\n",
       "[5 rows x 3228 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1,2), min_df=5, strip_accents='ascii', stop_words=['the', 'and', 'or', 'an'])\n",
    "count_vectorized = vectorizer.fit_transform(X_train['text'])\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "count_vectorized_ngram_df = pd.DataFrame(data=count_vectorized.toarray(), columns=columns)\n",
    "\n",
    "count_vectorized_row = count_vectorized.toarray()[0]\n",
    "print(f'Number of total features in training set: {len(count_vectorized_row)}')\n",
    "two_gram_features = [c for c in columns if len(c.split()) > 1]\n",
    "print(f'Number of 2-gram features in training set: {len(two_gram_features)}')\n",
    "print(f'Number of 1-gram features in training set: {len([c for c in columns if len(c.split()) == 1])}')\n",
    "\n",
    "print(f'10 2-gram features: {two_gram_features[:10]}')\n",
    "\n",
    "count_vectorized_ngram_df.head()\n",
    "\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dbc9e7",
   "metadata": {},
   "source": [
    "Development Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e78323b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total features in training set: 3228\n",
      "Number of 2-gram features in training set: 1250\n",
      "Number of 1-gram features in training set: 1978\n",
      "10 2-gram features: ['aba woman', 'abc news', 'able to', 'about it', 'about to', 'about trapped', 'according to', 'account of', 'action year', 'added video']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abcnews</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>able to</th>\n",
       "      <th>about</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth saved</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube playlist</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>youve</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  aba woman  abandoned  abc  abc news  abcnews  ablaze  able  able to  \\\n",
       "0    0          0          0    0         0        0       0     0        0   \n",
       "1    0          0          0    0         0        0       0     0        0   \n",
       "2    0          0          0    0         0        0       0     0        0   \n",
       "3    0          0          0    0         0        0       0     0        0   \n",
       "4    0          0          0    0         0        0       0     0        0   \n",
       "\n",
       "   about  ...  youth  youth saved  youtube  youtube playlist  youtube video  \\\n",
       "0      0  ...      0            0        0                 0              0   \n",
       "1      0  ...      0            0        0                 0              0   \n",
       "2      0  ...      0            0        0                 0              0   \n",
       "3      0  ...      0            0        0                 0              0   \n",
       "4      0  ...      0            0        0                 0              0   \n",
       "\n",
       "   youve  yr  yr old  yyc  zone  \n",
       "0      0   0       0    0     0  \n",
       "1      0   0       0    0     0  \n",
       "2      0   0       0    0     0  \n",
       "3      0   0       0    0     0  \n",
       "4      0   0       0    0     0  \n",
       "\n",
       "[5 rows x 3228 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorized_dev = vectorizer.transform(X_dev['text'])\n",
    "count_vectorized_ngram_dev_df = pd.DataFrame(data=count_vectorized_dev.toarray(), columns=columns)\n",
    "\n",
    "count_vectorized_row = count_vectorized.toarray()[0]\n",
    "print(f'Number of total features in training set: {len(count_vectorized_row)}')\n",
    "two_gram_features = [c for c in columns if len(c.split()) > 1]\n",
    "print(f'Number of 2-gram features in training set: {len(two_gram_features)}')\n",
    "print(f'Number of 1-gram features in training set: {len([c for c in columns if len(c.split()) == 1])}')\n",
    "\n",
    "print(f'10 2-gram features: {two_gram_features[:10]}')\n",
    "\n",
    "count_vectorized_ngram_dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83276a1",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4db32f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_l2 = LogisticRegression(penalty='l2').fit(count_vectorized_ngram_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398d376",
   "metadata": {},
   "source": [
    "Training Set F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4565798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8953117888029131\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l2.predict(count_vectorized_ngram_df)\n",
    "print(f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec86565",
   "metadata": {},
   "source": [
    "Development Set F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a31ef941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7506760411032991\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l2.predict(count_vectorized_ngram_dev_df)\n",
    "print(f1_score(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf4155",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a6e0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "psis, phis = get_naive_bayes_theta_vals(count_vectorized_ngram_df, y_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52475e02",
   "metadata": {},
   "source": [
    "Training Set F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0ac6ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7755\n"
     ]
    }
   ],
   "source": [
    "idx, logpyx = naive_bayes_predict(np.array(count_vectorized_ngram_df), psis, phis, 2)\n",
    "print(f1_score(y_train, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d8b97",
   "metadata": {},
   "source": [
    "Development Set F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb40d5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7397590361445783\n"
     ]
    }
   ],
   "source": [
    "idx, logpyx = naive_bayes_predict(np.array(count_vectorized_ngram_dev_df), psis, phis, 2)\n",
    "print(f1_score(y_dev, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b7064",
   "metadata": {},
   "source": [
    "For logistic regression, the model was able to predict the training set more accurately with the 2-gram model compared to the bag of words model and it performed similarly to the bag of words model for the development set.\n",
    "\n",
    "For Naive Bayes, the 2-gram model performed similarly on the training and develpoment sets compared to bag of words.\n",
    "\n",
    "This implies the Naive Bayes assumption stands true because words contribute independently to the performance of the model and as they became conditionally dependent, the results did not change significantly.\n",
    "\n",
    "Therefore, Naive Bayes generally performs well on this task because Naive Bayes assumes each word is independent of one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee445a",
   "metadata": {},
   "source": [
    "## i. Determine performance with the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8e2ff",
   "metadata": {},
   "source": [
    "We decided to use L2-regularized Logistic Regression with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "271cd99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accident</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1446 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  abandoned  abc  ablaze  able  about  absolutely  accident  according  \\\n",
       "0    0          0    0       0     0      0           0         0          0   \n",
       "1    0          0    0       0     0      0           0         0          0   \n",
       "2    0          0    0       0     0      0           0         0          0   \n",
       "3    0          0    0       0     0      0           0         0          0   \n",
       "4    0          0    0       0     0      0           0         0          0   \n",
       "\n",
       "   account  ...  young  your  youre  yours  yourself  youth  youtube  yr  yyc  \\\n",
       "0        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "1        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "2        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "3        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "4        0  ...      0     1      0      0         0      0        0   0    0   \n",
       "\n",
       "   zone  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 1446 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, min_df=10, strip_accents='ascii', stop_words=['the', 'and', 'or', 'an'])\n",
    "count_vectorized = vectorizer.fit_transform(X_total['text'])\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "count_vectorized_total_df = pd.DataFrame(data=count_vectorized.toarray(), columns=columns)\n",
    "count_vectorized_total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55c49f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accident</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1446 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aba  abandoned  abc  ablaze  able  about  absolutely  accident  according  \\\n",
       "0    0          0    0       0     0      0           0         0          0   \n",
       "1    0          0    0       0     0      1           0         0          0   \n",
       "2    0          0    0       0     0      0           0         0          0   \n",
       "3    0          0    0       0     0      0           0         0          0   \n",
       "4    0          0    0       0     0      0           0         0          0   \n",
       "\n",
       "   account  ...  young  your  youre  yours  yourself  youth  youtube  yr  yyc  \\\n",
       "0        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "1        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "2        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "3        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "4        0  ...      0     0      0      0         0      0        0   0    0   \n",
       "\n",
       "   zone  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 1446 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorized_test = vectorizer.transform(df_test['text'])\n",
    "count_vectorized_test_df = pd.DataFrame(data=count_vectorized_test.toarray(), columns=columns)\n",
    "count_vectorized_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f75ee023",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_l2 = LogisticRegression(penalty='l2').fit(count_vectorized_total_df, y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "178e2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = clf_l2.predict(count_vectorized_test_df)\n",
    "y_pred_final = pd.DataFrame(data = y_pred, index = test_id_cols, columns = ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62618cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CSV with predictions\n",
    "y_pred_final.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ee423",
   "metadata": {},
   "source": [
    "When we submitted to Kaggle, we got a score of .79466"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88444f",
   "metadata": {},
   "source": [
    "This was higher than we expected because it is a better score than our tests on the development set.  However, we trained using both the traning and development sets so it makes sense that our Kaggle predictions were higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ed033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
